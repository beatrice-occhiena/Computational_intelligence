{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Beatrice Occhiena s314971. See [`LICENSE`](https://github.com/beatrice-occhiena/Computational_intelligence/blob/main/LICENSE) for details.\n",
    "- institutional email: `S314971@studenti.polito.it`\n",
    "- personal email: `beatrice.occhiena@live.it`\n",
    "- github repository: [https://github.com/beatrice-occhiena/Computational_intelligence.git](https://github.com/beatrice-occhiena/Computational_intelligence.git)\n",
    "\n",
    "**Resources:** These notes are the result of additional research and analysis of the lecture material presented by Professor Giovanni Squillero for the Computational Intelligence course during the academic year 2023-2024 @ Politecnico di Torino. They are intended to be my attempt to make a personal contribution and to rework the topics covered in the following resources.\n",
    "- [https://github.com/squillero/computational-intelligence](https://github.com/squillero/computational-intelligence)\n",
    "- Stuart Russel, Peter Norvig, *Artificial Intelligence: A Modern Approach* [3th edition]\n",
    "- Richard S. Sutton, Andrew G. Barto, *Reinforcement Learning: An Introduction* [2nd Edition]\n",
    "- Useful site to better understand Monte Carlo method [analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/)\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ML paradigm where an agent learns to make sequential decisions by interacting with an environment, receiving feedback in the form of rewards, with the objective of *maximizing its cumulative long-term performance*.\n",
    "\n",
    "### Key concepts\n",
    "- **Agent and Environment Interaction**: an agent performs actions that modify the environment and, in return, receives a reward for each action. The state of the environment and the reward are updated with each step creating the following cycle.\n",
    "\n",
    "  The agent:\n",
    "    1. Receive an `observation` $S^A_t$ from the environment\n",
    "    2. Select an `action` $A_t$ based on the observation\n",
    "    3. Execute the action $A_t$ and receive a reward $R_t$\n",
    "\n",
    "  The environment:\n",
    "    1. Receive an action $A_t$ from the agent\n",
    "    2. Execute the action $A_t$ and update its `state` to $S^E_{t+1}$\n",
    "    3. Calculate the `reward` $R_{t}$ and send it to the agent\n",
    "\n",
    "- **Environment State and Observability**: the state of the environment is a representation of the current situation of the environment. It is a function of the `history` of all the previous actions and observations. The agent does not have direct access to the state of the environment, but only to the observation $S^A_t$.\n",
    "\n",
    "  An environment is **fully observable** if the agent can directly observe all aspects of the environment ($S^A_t = S^E_t$).\n",
    "  - Chess â™Ÿï¸\n",
    "  - Tic-tac-toe âŒâ­•\n",
    "  - Pacman ğŸ±\n",
    "\n",
    "  In contrast, in **partially observable** environments, if the agent receives indirect information about the environment.\n",
    "  - Poker ğŸƒ\n",
    "  - Self-driving car ğŸš—\n",
    "  - Stock market ğŸ“ˆ\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
